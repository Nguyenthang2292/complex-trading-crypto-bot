import os
import sys
import pandas as pd
from pathlib import Path
import glob
from tqdm import tqdm

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from components.load_all_symbols_data import load_all_symbols_data
from components.tick_processor import TickProcessor
from components.combine_all_dataframes import combine_all_dataframes
from config.config import DEFAULT_TIMEFRAMES, SIGNAL_LONG, SIGNAL_SHORT
from signals.quant_models.random_forest.signals_random_forest import (
    get_latest_random_forest_signal, train_and_save_global_rf_model, load_random_forest_model
)
from signals.quant_models.best_performance_symbols.signals_best_performance_symbols import signal_best_performance_symbols
from signals.quant_models.hmm.signals_hmm import hmm_signals
from utilities.logger import setup_logging


logger = setup_logging(module_name="simplified_trading_signal_analyzer", log_level=20)

def normalize_columns(df):
    """ƒê·∫£m b·∫£o DataFrame c√≥ c·∫£ c·ªôt vi·∫øt hoa v√† th∆∞·ªùng cho OHLCV."""
    for col in ['open', 'high', 'low', 'close', 'volume']:
        if col in df.columns and col.upper() not in df.columns:
            df[col.upper()] = df[col]
        if col.upper() in df.columns and col not in df.columns:
            df[col] = df[col.upper()]
    return df

def clean_all_symbols_data(all_symbols_data, min_len=48):
    cleaned = {}
    for sym, tf_dict in all_symbols_data.items():
        if isinstance(tf_dict, dict):
            cleaned_tf = {tf: df for tf, df in tf_dict.items()
                          if isinstance(df, pd.DataFrame) and len(df) >= min_len}
            if cleaned_tf:
                cleaned[sym] = cleaned_tf
    return cleaned

def main():
    # B∆∞·ªõc 1: Kh·ªüi t·∫°o v√† n·∫°p d·ªØ li·ªáu
    logger.process("B∆∞·ªõc 1: Kh·ªüi t·∫°o processor v√† n·∫°p d·ªØ li·ªáu l·ªãch s·ª≠")
    processor = TickProcessor(trade_open_callback=None, trade_close_callback=None)
    symbols = processor.get_symbols_list_by_quote_usdt()
    logger.info(f"T·ªïng s·ªë c·∫∑p USDT: {len(symbols)}")
    all_symbols_data = load_all_symbols_data(processor, symbols, timeframes=DEFAULT_TIMEFRAMES)
    if all_symbols_data is not None:
        for sym, tf_dict in all_symbols_data.items():
            if tf_dict is not None and isinstance(tf_dict, dict):
                for tf, df in tf_dict.items():
                    if isinstance(tf, str) and isinstance(df, pd.DataFrame):
                        tf_dict[tf] = normalize_columns(df)
        all_symbols_data = clean_all_symbols_data(all_symbols_data, min_len=48)
        logger.success("N·∫°p d·ªØ li·ªáu xong.")
        logger.info(f"S·ªë l∆∞·ª£ng symbols sau clean_all_symbols_data: {len(all_symbols_data)}")
        for sym, tf_dict in all_symbols_data.items():
            logger.info(f"Symbol: {sym}, s·ªë timeframe: {len(tf_dict)}, timeframes: {list(tf_dict.keys())}")
    else:
        logger.error("Kh√¥ng load ƒë∆∞·ª£c all_symbols_data!")

    # B∆∞·ªõc 2: Ph√¢n t√≠ch hi·ªáu su·∫•t c√°c c·∫∑p s·ª≠ d·ª•ng signal_best_performance_symbols
    logger.analysis("B∆∞·ªõc 2: Ph√¢n t√≠ch hi·ªáu su·∫•t c√°c c·∫∑p b·∫±ng signal_best_performance_symbols")
    perf_result = signal_best_performance_symbols(
        processor=processor,
        symbols=symbols,
        timeframes=DEFAULT_TIMEFRAMES,
        performance_period=48,  # TƒÉng t·ª´ 24 l√™n 48 ƒë·ªÉ c√≥ nhi·ªÅu d·ªØ li·ªáu h∆°n
        top_percentage=0.3,     # Gi·∫£m t·ª´ 0.4 xu·ªëng 0.3 ƒë·ªÉ l·ªçc ch·∫∑t h∆°n
        include_short_signals=True,
        worst_percentage=0.3,   # Gi·∫£m t·ª´ 0.4 xu·ªëng 0.3 ƒë·ªÉ l·ªçc ch·∫∑t h∆°n
        min_volume_usdt=500000, # TƒÉng t·ª´ 100k l√™n 500k ƒë·ªÉ ƒë·∫£m b·∫£o thanh kho·∫£n
        exclude_stable_coins=True,
        preloaded_data=all_symbols_data
    )
    top_long = perf_result.get('best_performers_long', [])
    top_short = perf_result.get('worst_performers_short', [])
    logger.info(f"S·ªë l∆∞·ª£ng best_performers_long: {len(top_long)}")
    logger.info(f"S·ªë l∆∞·ª£ng worst_performers_short: {len(top_short)}")
    logger.info(f"Top LONG: {[item['symbol'] for item in top_long]}")
    logger.info(f"Top SHORT: {[item['symbol'] for item in top_short]}")

    # L·ªçc l·∫°i d·ªØ li·ªáu ch·ªâ gi·ªØ c√°c c·∫∑p t·ªët nh·∫•t/k√©m nh·∫•t
    long_symbols = set([item['symbol'] for item in top_long])
    short_symbols = set([item['symbol'] for item in top_short])
    filtered_data_long = {s: all_symbols_data[s] for s in long_symbols if s in all_symbols_data}
    filtered_data_short = {s: all_symbols_data[s] for s in short_symbols if s in all_symbols_data}
    logger.info(f"S·ªë l∆∞·ª£ng filtered_data_long: {len(filtered_data_long)}")
    logger.info(f"S·ªë l∆∞·ª£ng filtered_data_short: {len(filtered_data_short)}")

    # B∆∞·ªõc 3: K·∫øt h·ª£p RF v√† HMM
    logger.process("B∆∞·ªõc 3: Hu·∫•n luy·ªán v√† sinh t√≠n hi·ªáu Random Forest")
    # X√≥a c√°c file rf_model_global* trong th∆∞ m·ª•c models tr∆∞·ªõc khi train l·∫°i
    models_dir = Path(__file__).parent / "models"
    rf_model_files = glob.glob(str(models_dir / "rf_model_global*"))
    if rf_model_files:
        for f in rf_model_files:
            try:
                os.remove(f)
                logger.info(f"ƒê√£ x√≥a file model c≈©: {f}")
            except Exception as e:
                logger.error(f"Kh√¥ng th·ªÉ x√≥a file model: {f}, l·ªói: {e}")
    else:
        logger.info("Kh√¥ng c√≥ file rf_model_global c≈© n√†o ƒë·ªÉ x√≥a.")

    combined_df = combine_all_dataframes(all_symbols_data)
    logger.info(f"S·ªë l∆∞·ª£ng d√≤ng combined_df: {len(combined_df) if combined_df is not None else 0}")
    rf_model, rf_model_path = train_and_save_global_rf_model(combined_df, model_filename="rf_model_global.joblib")
    rf_model = load_random_forest_model(Path(rf_model_path))
    rf_signals_long, rf_signals_short = [], []

    if rf_model is not None:
        # L·ªçc LONG
        for item in top_long:
            sym = item['symbol']
            long_score = item.get('long_score', 0)
            if sym in filtered_data_long:
                for tf, df in filtered_data_long[sym].items():
                    rf_signal, rf_confidence = get_latest_random_forest_signal(df, rf_model)
                    if rf_signal == SIGNAL_LONG:
                        rf_signals_long.append({
                            'symbol': sym,
                            'timeframe': tf,
                            'rf_signal': rf_signal,
                            'confidence_rf': rf_confidence,
                            'long_score': long_score
                        })
        # L·ªçc SHORT
        for item in top_short:
            sym = item['symbol']
            short_score = item.get('short_score', 0)
            if sym in filtered_data_short:
                for tf, df in filtered_data_short[sym].items():
                    rf_signal, rf_confidence = get_latest_random_forest_signal(df, rf_model)
                    if rf_signal == SIGNAL_SHORT:
                        rf_signals_short.append({
                            'symbol': sym,
                            'timeframe': tf,
                            'rf_signal': rf_signal,
                            'confidence_rf': rf_confidence,
                            'short_score': short_score
                        })
    else:
        logger.error("Kh√¥ng th·ªÉ load m√¥ h√¨nh Random Forest. B·ªè qua b∆∞·ªõc sinh t√≠n hi·ªáu RF.")

    logger.info(f"rf_signals_long: {rf_signals_long}")
    logger.info(f"rf_signals_short: {rf_signals_short}")
    logger.info(f"S·ªë c·∫∑p LONG qua RF: {len(rf_signals_long)}")
    logger.info(f"S·ªë c·∫∑p SHORT qua RF: {len(rf_signals_short)}")

    # Ch·∫°y HMM tr√™n c√°c c·∫∑p ƒë√£ l·ªçc (b·ªè qua confidence HMM)
    logger.process("Ch·∫°y HMM tr√™n c√°c c·∫∑p ƒë√£ l·ªçc")
    hmm_signals_long, hmm_signals_short = [], []
    for entry in tqdm(rf_signals_long, desc="HMM LONG", ncols=80):
        df = all_symbols_data[entry['symbol']][entry['timeframe']]
        logger.debug(f"Processing HMM LONG - {entry['symbol']} {entry['timeframe']}: shape={df.shape}, index_type={type(df.index)}")
        hmm_result = hmm_signals(df)
        logger.info(f"HMM signal LONG - Symbol: {entry['symbol']}, TF: {entry['timeframe']}, HMM result: {hmm_result}")
        if hmm_result[0] == 1 or hmm_result[1] == 1:  # strict ho·∫∑c non-strict LONG
            hmm_signals_long.append(entry)
    for entry in tqdm(rf_signals_short, desc="HMM SHORT", ncols=80):
        df = all_symbols_data[entry['symbol']][entry['timeframe']]
        logger.debug(f"Processing HMM SHORT - {entry['symbol']} {entry['timeframe']}: shape={df.shape}, index_type={type(df.index)}")
        hmm_result = hmm_signals(df)
        logger.info(f"HMM signal SHORT - Symbol: {entry['symbol']}, TF: {entry['timeframe']}, HMM result: {hmm_result}")
        if hmm_result[0] == -1 or hmm_result[1] == -1:  # strict ho·∫∑c non-strict SHORT
            hmm_signals_short.append(entry)

    logger.info(f"hmm_signals_long: {hmm_signals_long}")
    logger.info(f"hmm_signals_short: {hmm_signals_short}")
    logger.info(f"S·ªë c·∫∑p LONG ƒë·ªìng thu·∫≠n RF+HMM: {len(hmm_signals_long)}")
    logger.info(f"S·ªë c·∫∑p SHORT ƒë·ªìng thu·∫≠n RF+HMM: {len(hmm_signals_short)}")

    # B∆∞·ªõc 4: L·ªçc t√≠n hi·ªáu cu·ªëi c√πng
    logger.process("B∆∞·ªõc 4: L·ªçc t√≠n hi·ªáu cu·ªëi c√πng v√† chu·∫©n h√≥a format")
    final_long = [
        {
            'pair': e['symbol'],
            'direction': 'LONG',
            'timeframe': e['timeframe'],
            'confidence_rf': e['confidence_rf'],
            'long_score': e['long_score'],
            'confidence_total': (e['confidence_rf'] + e['long_score']) / 2
        }
        for e in hmm_signals_long if e['symbol'] in long_symbols
    ]
    final_short = [
        {
            'pair': e['symbol'],
            'direction': 'SHORT',
            'timeframe': e['timeframe'],
            'confidence_rf': e['confidence_rf'],
            'short_score': e['short_score'],
            'confidence_total': (e['confidence_rf'] + e['short_score']) / 2
        }
        for e in hmm_signals_short if e['symbol'] in short_symbols
    ]

    # S·∫Øp x·∫øp theo confidence t·ªïng h·ª£p gi·∫£m d·∫ßn
    final_long_sorted = sorted(final_long, key=lambda x: x.get('confidence_total', 0.0), reverse=True)
    final_short_sorted = sorted(final_short, key=lambda x: x.get('confidence_total', 0.0), reverse=True)

    logger.info("Top 10 t√≠n hi·ªáu LONG cu·ªëi c√πng:")
    if final_long_sorted:
        for i, entry in enumerate(final_long_sorted[:10], 1):
            logger.info(
                f"  {i:2d}. üü¢ {entry['pair']:12s} | Direction: {entry['direction']:<5s} | TF: {entry['timeframe']:<2s} | "
                f"RF: {entry['confidence_rf']:.2%} | Perf: {entry['long_score']:.2%} | T·ªïng h·ª£p: {entry['confidence_total']:.2%}"
            )
    else:
        logger.info("  Kh√¥ng c√≥ t√≠n hi·ªáu LONG n√†o.")

    logger.info("Top 10 t√≠n hi·ªáu SHORT cu·ªëi c√πng:")
    if final_short_sorted:
        for i, entry in enumerate(final_short_sorted[:10], 1):
            logger.info(
                f"  {i:2d}. üî¥ {entry['pair']:12s} | Direction: {entry['direction']:<5s} | TF: {entry['timeframe']:<2s} | "
                f"RF: {entry['confidence_rf']:.2%} | Perf: {entry['short_score']:.2%} | T·ªïng h·ª£p: {entry['confidence_total']:.2%}"
            )
    else:
        logger.info("  Kh√¥ng c√≥ t√≠n hi·ªáu SHORT n√†o.")

    # B∆∞·ªõc 5: Logging & Debug
    logger.process("B∆∞·ªõc 5: Debug n√¢ng cao")
    logger.info(f"S·ªë l∆∞·ª£ng c·∫∑p tr√πng nhau gi·ªØa c√°c t·∫ßng l·ªçc LONG: {len(set([e['pair'] for e in final_long]))}")
    logger.info(f"S·ªë l∆∞·ª£ng c·∫∑p tr√πng nhau gi·ªØa c√°c t·∫ßng l·ªçc SHORT: {len(set([e['pair'] for e in final_short]))}")

if __name__ == "__main__":
    main()